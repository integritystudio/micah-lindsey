---
layout: single
title: "Trust & Safety Meets AI Governance: Why Platform Integrity Professionals Are Essential to the EU AI Act"
date: 2026-01-31
categories: [trust-and-safety]
tags: [EU AI Act, AI governance, trust and safety, platform integrity, fraud prevention]
excerpt: "The EU AI Act demands professionals who understand abuse patterns, enforcement at scale, and the gap between policy and platform reality. Here's why trust and safety expertise is essential to AI governance."
description: "How trust and safety professionals can leverage platform integrity experience for AI governance careers."
schema_type: tech-article
author_profile: true
toc: true
toc_label: "On This Page"
toc_sticky: true
header:
  overlay_color: "#1a1a2e"
  overlay_filter: "0.5"
---

When I transitioned from investigating business integrity threats at Meta to working on AI governance at a tech startup, I expected a steep learning curve on the regulatory side. What I didn't expect was how much my trust and safety training would become the foundation for everything else.

The EU AI Act isn't just a regulatory compliance challenge—it's a platform integrity problem. And platform integrity is something trust and safety professionals have studied, practiced, and refined for years. The skills that make someone effective at identifying bad actors—pattern recognition, data pipeline development, cross-functional enforcement—turn out to be exactly what AI governance demands.

## Why Trust & Safety Expertise Matters in AI Governance

The AI Act is, at its core, a piece of legislation about preventing harm. It was drafted to address risks from AI systems—risks that trust and safety professionals have been managing on platforms for over a decade. Yet much of the compliance conversation focuses on purely legal solutions: documentation, audits, risk assessments.

This compliance-only focus misses something crucial. The AI Act identifies over 80 specific obligations for high-risk AI systems—each requiring not just legal interpretation, but operational implementation. Before you can build compliant systems, you need to understand what abuse looks like, how bad actors adapt, and how enforcement actually works at scale. That requires the investigative mindset that trust and safety professionals develop through years of practice.

Consider Article 9's requirement for "risk management systems." The text calls for processes that identify, analyze, and mitigate risks throughout the AI system lifecycle. A purely legal reading might focus on documentation frameworks. A trust and safety reading asks different questions: What does abuse actually look like in this system? How will bad actors exploit gaps? What signals should trigger investigation?

These aren't legal questions. They're operational integrity questions. And answering them requires the investigative skills that trust and safety professionals develop through experience.

## The Enforcement Problem

Every governance framework faces what I think of as the enforcement problem: the gap between rules on paper and enforcement in practice. Regulators write in abstractions—"appropriate levels of accuracy," "effective human oversight," "transparent operations." Organizations must convert these abstractions into specific, measurable, enforceable practices.

This translation work is inherently operational. It requires understanding both the regulatory context and the technical reality of how systems are abused. Neither lawyers nor engineers can do this translation alone.

In my experience at Meta, the most effective integrity work happened when investigators, engineers, and policy teams worked as genuine partners. Investigators brought pattern recognition, data analysis skills, and understanding of how enforcement actually works. Engineers brought system architecture knowledge. Together, they could develop enforcement approaches that were both policy-compliant and operationally effective.

## Lessons from Business Integrity at Meta

My time investigating business integrity threats at Meta taught me several things that prove directly relevant to AI Act compliance:

### Patterns of Abuse Evolve Constantly

Bad actors don't stand still. At Meta, we saw constant evolution in how advertisers abused the platform—new techniques, new evasion strategies, new attack vectors. The same will be true for AI systems under the EU AI Act. Organizations that build static compliance frameworks will find them outdated quickly.

Trust and safety professionals are trained to think about abuse dynamically. We build detection systems that evolve, enforcement pipelines that adapt, and investigation workflows that can handle novel threats.

### Actor-Level Enforcement Changes Everything

One of the most impactful shifts during my time at Meta was moving from content-level to actor-level enforcement. Instead of playing whack-a-mole with individual violations, we identified the actors behind them and addressed the root cause.

This same principle applies to AI governance. The EU AI Act focuses heavily on providers and deployers—the actors responsible for AI systems. Organizations that can identify, investigate, and enforce at the actor level will be far more effective at compliance than those that treat each obligation in isolation.

### Data Pipelines Are the Foundation

Effective trust and safety work depends on reliable data infrastructure. At Meta, I created and maintained data pipelines that automated the aggregation of signals and enforcement of bad actors. This same infrastructure thinking is essential for AI governance.

The AI Act requires ongoing monitoring, logging, and reporting. Organizations need data pipelines that can aggregate compliance signals, flag anomalies, and support investigation workflows. Trust and safety professionals know how to build these systems.

## Building Trust & Safety Capacity for AI Governance

For organizations taking AI Act compliance seriously, building internal trust and safety capacity is as important as building legal infrastructure. This means:

### Hiring Investigative Expertise

AI governance teams need people who can identify abuse patterns, build enforcement systems, and investigate violations. This might mean hiring trust and safety investigators, fraud analysts, or integrity specialists. The specific title matters less than the skill set: investigative rigor, data analysis capability, and the ability to think like both an attacker and a defender.

### Creating Cross-Functional Processes

Compliance can't be siloed. Legal teams need ongoing input from trust and safety experts who understand how systems are actually abused. Engineering teams need input on what signals to capture and what enforcement mechanisms to build. Organizations should build integrated teams that facilitate this collaboration.

### Developing Detection Frameworks

Rather than treating each compliance question in isolation, organizations benefit from developing coherent detection frameworks that can identify potential violations across systems. What signals indicate a high-risk AI system is being misused? How will you investigate anomalies? What enforcement actions are available?

These frameworks should be developed by people with investigative experience—people who know what abuse looks like in practice, not just in theory.

## The Opportunity for Trust & Safety Professionals

The AI Act represents an inflection point for trust and safety careers. As AI governance becomes a permanent feature of the regulatory landscape, organizations will need sustained investigative expertise—not just for initial compliance, but for ongoing detection and enforcement as AI systems evolve.

For trust and safety professionals interested in governance, this is an extraordinary opportunity. The field is new enough that operational integrity skills—data analysis, pattern recognition, enforcement pipeline development, cross-functional collaboration—provide genuine value. You don't need a law degree to contribute meaningfully to AI governance. You need the ability to identify abuse, investigate it systematically, and build enforcement systems that scale.

The gap between regulatory intent and operational enforcement won't bridge itself. That's work for people who understand both sides—and who can help organizations build AI systems that are not just compliant, but genuinely safe.

---

*Micah Lindsey is a Trust & Safety Professional at [Integrity Studio](https://integritystudio.ai), where he brings over 15 years of platform integrity and fraud prevention experience to AI governance. He previously served as a Business Integrity Technical Investigator at Meta for over 8 years, specializing in actor-level enforcement and data pipeline development. [Connect on LinkedIn](https://linkedin.com/in/micahlindsey)*
